{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 20)\n",
      "(8522, 20)\n"
     ]
    }
   ],
   "source": [
    "train_ = df.iloc[:40000]\n",
    "test_ = df.iloc[40000:]\n",
    "print(train_.shape)\n",
    "print(test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_story_tfidf = pd.read_pickle(\"../data/train_tfidf.pkl\").reset_index(drop=True)\n",
    "test_story_tfidf = pd.read_pickle(\"../data/test_tfidf.pkl\").reset_index(drop=True)\n",
    "print(train_story_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFを試す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Load data!\n"
     ]
    }
   ],
   "source": [
    "# 前処理用\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import spacy\n",
    "import neologdn\n",
    "import json\n",
    "\n",
    "# 分かち書き用\n",
    "import ginza\n",
    "import ja_ginza_electra\n",
    "# pandas高速化\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import json\n",
    "import os\n",
    "import emoji\n",
    "import mojimoji\n",
    "import neologdn\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Load data!\")\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i in train[\"story\"]:\n",
    "    if type(i) != str:\n",
    "        print(i)\n",
    "        l.append(type(i))\n",
    "    else:\n",
    "        pass\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_json_path = \"../emoji/emoji_ja.json\"\n",
    "json_open = open(emoji_json_path)\n",
    "emoji_dict = json.load(json_open)\n",
    "\n",
    "def wakati_rm_func(x):\n",
    "    sentence = x\n",
    "    nlp = spacy.load('ja_ginza_electra')\n",
    "    sentence = re.sub(r'[!-~]',\" \",sentence) # 小文字の記号を削除\n",
    "    sentence=re.sub(r'[︰-＠]', \"\", sentence) # 大文字の記号を削除\n",
    "\n",
    "\n",
    "    # 不要記号削除\n",
    "    pattern = '[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”◇ᴗ●↓→♪★⊂⊃※△□◎〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％�]'\n",
    "    sentence =  re.sub(pattern, ' ', sentence)\n",
    "\n",
    "    # 正規化する\n",
    "    sentence = neologdn.normalize(sentence)\n",
    "\n",
    "    # 大文字・小文字変換\n",
    "    sentence = sentence.replace(\"\\n\", \"\")\n",
    "    sentence = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", \"\", sentence)\n",
    "    # 絵文字削除\n",
    "    sentence = \"\".join(\n",
    "        [\n",
    "            \"絵文字\" + emoji_dict.get(c, {\"short_name\": \"\"}).get(\"short_name\", \"\")\n",
    "            if c in emoji.UNICODE_EMOJI[\"en\"]\n",
    "            else c\n",
    "            for c in sentence\n",
    "        ]\n",
    "    )\n",
    "    # GinZaで分かち書きをする\n",
    "    doc = nlp(sentence)\n",
    "    tmp_words_list = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ in [\"PROPN\", \"NOUN\", \"ADJ\", \"VERB\"]:\n",
    "                tmp_words_list.append(token.orth_)\n",
    "\n",
    "    result = \" \".join(tmp_words_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'高木 さん'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wakati_rm_func(\"今日も高木さん\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/usr/local/lib/python3.8/site-packages/pandarallel/pandarallel.py\", line 64, in global_worker\n    return _func(x)\n  File \"/usr/local/lib/python3.8/site-packages/pandarallel/pandarallel.py\", line 108, in wrapper\n    result = function(\n  File \"/usr/local/lib/python3.8/site-packages/pandarallel/data_types/dataframe.py\", line 31, in worker\n    return df.apply(func, *args, **kwargs)\n  File \"/usr/local/lib/python3.8/site-packages/pandas/core/frame.py\", line 8740, in apply\n    return op.apply()\n  File \"/usr/local/lib/python3.8/site-packages/pandas/core/apply.py\", line 688, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.8/site-packages/pandas/core/apply.py\", line 812, in apply_standard\n    results, res_index = self.apply_series_generator()\n  File \"/usr/local/lib/python3.8/site-packages/pandas/core/apply.py\", line 828, in apply_series_generator\n    results[i] = self.f(v)\n  File \"/tmp/ipykernel_98335/3015723636.py\", line 8, in wakati_rm_func\n    sentence = re.sub(r'[!-~]',\" \",sentence) # 小文字の記号を削除\n  File \"/usr/local/lib/python3.8/re.py\", line 210, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or bytes-like object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98335/4067208399.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# trainに分かち書きを実行する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title_wakati\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwakati_rm_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"story_wakati\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"story\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwakati_rm_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keyword_wakati\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keyword\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwakati_rm_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(data, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             results = get_workers_result(\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0muse_memory_fs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0mnb_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mget_workers_result\u001b[0;34m(use_memory_fs, nb_workers, show_progress_bar, nb_columns, queue, chunk_lengths, input_files, output_files, map_result)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mprogress_bars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogresses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     return (\n",
      "\u001b[0;32m/usr/local/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train, test])\n",
    "# trainに分かち書きを実行する\n",
    "df[\"title_wakati\"] = df[[\"title\"]].parallel_apply(wakati_rm_func)\n",
    "df[\"story_wakati\"] = df[[\"story\"]].parallel_apply(wakati_rm_func)\n",
    "df[\"keyword_wakati\"] = df[[\"keyword\"]].parallel_apply(wakati_rm_func)\n",
    "\n",
    "train_ = df.iloc[:40000]\n",
    "test_ = df.iloc[40000:]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train[\"story\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 20)\n",
      "(8522, 19)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "         ..\n",
       "39995   NaN\n",
       "39996   NaN\n",
       "39997   NaN\n",
       "39998   NaN\n",
       "39999   NaN\n",
       "Name: story_wakati, Length: 40000, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_[\"story_wakati\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>欠損</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        欠損\n",
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      1.0\n",
       "...    ...\n",
       "39995  1.0\n",
       "39996  1.0\n",
       "39997  1.0\n",
       "39998  1.0\n",
       "39999  1.0\n",
       "\n",
       "[120000 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TI-IDFを計算する\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model = TfidfVectorizer()\n",
    "X = model.fit_transform(train[\"story_wakati\"])\n",
    "story_tfidf_train = pd.DataFrame(data= X.toarray(), columns = model.get_feature_names())\n",
    "\n",
    "model = TfidfVectorizer()\n",
    "X = model.fit_transform(train[\"title_wakati\"])\n",
    "title_tfidf_train = pd.DataFrame(data= X.toarray(), columns = model.get_feature_names())\n",
    "\n",
    "model = TfidfVectorizer()\n",
    "X = model.fit_transform(train[\"keyword_wakati\"])\n",
    "keyword_tfidf_train = pd.DataFrame(data= X.toarray(), columns = model.get_feature_names())\n",
    "\n",
    "pd.concat([story_tfidf_train, title_tfidf_train, keyword_tfidf_train])#.to_pickle(\"data/train_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfVectorizer()\n",
    "X = model.fit_transform(test[\"story_wakati\"])\n",
    "story_tfidf_test = pd.DataFrame(data= X.toarray(), columns = model.get_feature_names())\n",
    "\n",
    "model = TfidfVectorizer()\n",
    "X = model.fit_transform(test[\"title_wakati\"])\n",
    "title_tfidf_test = pd.DataFrame(data= X.toarray(), columns = model.get_feature_names())\n",
    "\n",
    "model = TfidfVectorizer()\n",
    "X = model.fit_transform(test[\"keyword_wakati\"])\n",
    "keyword_tfidf_test = pd.DataFrame(data= X.toarray(), columns = model.get_feature_names())\n",
    "print(story_tfidf_test.shape)\n",
    "pd.concat([story_tfidf_test, title_tfidf_test, keyword_tfidf_test]).to_pickle(\"data/test_tfidf.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
